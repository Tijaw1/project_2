# Are You Worthy? 

### **Team Members**
- April
- Endalkachew
- Matt
- Ryan

### **Project Description**

- Utilizing personal data submitted by applicants, we’re going to use (XYZ Model) to quantify the magnitude of risk and predict ‘creditworthiness” to reject or approve issuing credit cards. We will then create a chat bot to assess an provide and immediate decision to an application. 

### **Objectives / Project Questions to Answer**
- Define ‘good’ or ‘bad’ credit
- What is the threshold to accept or reject an application?
- What is the best model(s) to try and predict this type of outcome?
- What’re some odd data points with high relevancy to creditworthiness? (optional/stretch goal)
- Is there correlation to Gender, Age, Income, Number of years employed etc on the approval for a credit card?

### **Data Sources**
- [Kaggle](https://www.kaggle.com/rikdifos/credit-card-approval-prediction)
- [ICS.edu](https://archive.ics.uci.edu/ml/datasets/credit+approval)
- [US Federal Reserve](https://www.federalreserve.gov/releases/g19/current/default.htm)

### **Rough Breakdown of Tasks**
- Select realm of Project Type (Algos, NLP, ML, etc)
  - Classification/Machine Learning?
- Select Dataset for baseline analysis
- Identify Features of Creditworthiness
  - Run classification on features of data set to identify statistically significant features of the data to make      decisions on.
- Define Research/Project Outcome Objectives
  - Build Credit Scoring Model
  - Build Bot to interact with Clients in Real Time Credit Decisions Based on our Scoring Model
  - Display understanding of New ML Model/Library
- Load Data into Google Collab Notebook
  - https://colab.research.google.com/drive/1ZANriFDfMgfxqpOxnZMHuMNxAcY48Az8?usp=sharing
- Clean the Dataset
- Transform the dataset to be useable
- Define the model 
- Train/test split the data
- Check fit
- Train the model
- Review outcome
- Define Machine Learning or Statistical Models for Related Project Outcome
- Build Models and artfully adjust them
  - Load the Data
  - Clean the Data
  - Pick the Model
  - Fit
  - Train/Test/Split
- Build Amazon Lex Bot
  - Create IAM users for Team (Ryan)
  - Build Framework for BOT with all available dataset features
  - Minimize number of features based on statistical significance (April) 


